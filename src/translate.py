#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time : 2025/8/10
# @Author : julong@111.com
# @Description : JuSubTitleAutoTranslate - SRTå­—å¹•æ–‡ä»¶AIè‡ªåŠ¨ç¿»è¯‘è„šæœ¬
#   æ”¯æŒ Helsinki-NLP/opus-mt-en-zhï¼ˆé€Ÿåº¦å¿«ï¼‰å’Œfacebook/nllb-200-distilled-600Mï¼ˆè´¨é‡é«˜ï¼‰ä¸¤ç§æ¨¡å‹

import argparse
import logging
import sys
import time
from pathlib import Path

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from language_code_utils import (
    nllb_language_codes,
    # ISO_639_2T_to_ISO_639_1,
    # ISO_15924_to_ISO_3166_1,
    # ISO_639_2T_ISO_15924_to_ISO_639_1_ISO_3166_1,
    # ISO_639_1_to_ISO_639_2T,
    # ISO_3166_1_to_ISO_15924,
    # ISO_639_1_ISO_3166_1_to_ISO_639_2T_ISO_15924
)
from srt_utils import (
    clean_text,
    get_output_file,
    parse_srt_file,
    validate_file_format,
    write_srt_file,
)

# æ–°å¢ï¼šè°ƒè¯•æ¨¡å¼ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½å’Œç¿»è¯‘
debug_skip = False

TRANSLATES_MODEL_MAP = {
    "opus": "Helsinki-NLP/opus-mt-en-zh",
    "nllb": "facebook/nllb-200-distilled-600M",
}


# æ–°å¢çš„å‡½æ•°ï¼Œç”¨äºå¤„ç†å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰é€»è¾‘
def process_single_file(input_path, output_path, args, tokenizer, model):
    """
    å¤„ç†å•ä¸ªSRTæ–‡ä»¶çš„å®Œæ•´æµç¨‹ï¼šè§£æã€ç¿»è¯‘ã€ä¿å­˜ã€‚
    """
    try:
        logging.info("ğŸ“– æ­£åœ¨è§£ææ–‡ä»¶...")
        subtitles = parse_srt_file(input_path)
        logging.info(f"âœ… è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(subtitles)} æ¡å­—å¹•")
        elapsed_time = 1
        # ç¿»è¯‘å­—å¹•
        if debug_skip:
            logging.info("ğŸ è°ƒè¯•æ¨¡å¼å·²å¼€å¯: è·³è¿‡ç¿»è¯‘æ­¥éª¤ã€‚")
            translated_count = 0
            # ä¿æŒåŸå§‹æ–‡æœ¬ä¸å˜
            for i, subtitle in enumerate(subtitles):
                subtitle["text"] = subtitle["text"]
                translated_count += 1
        else:
            # ç¿»è¯‘å­—å¹•
            logging.info(f"ğŸŒ æ­£åœ¨ä½¿ç”¨{args.model}æ¨¡å‹ç¿»è¯‘å­—å¹•...")

            translated_count = 0
            start_time = time.time()

            for i, subtitle in enumerate(subtitles):
                logging.info(f"ç¿»è¯‘è¿›åº¦: {i + 1}/{len(subtitles)}")

                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ€§åœ°ä¼ é€’å‚æ•°
                if args.model == "nllb":
                    # NLLBæ¨¡å‹éœ€è¦æŒ‡å®šæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
                    input_text = f"{args.source_lang} {subtitle['text']}"

                    inputs = tokenizer(
                        input_text,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=args.max_length,
                    )
                    outputs = model.generate(
                        **inputs,
                        forced_bos_token_id=tokenizer.convert_tokens_to_ids(
                            args.target_lang
                        ),
                    )
                    translated_text = tokenizer.decode(
                        outputs[0], skip_special_tokens=True
                    )
                else:  # 'opus'
                    inputs = tokenizer(
                        subtitle["text"],
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=args.max_length,
                    )
                    outputs = model.generate(**inputs)
                    translated_text = tokenizer.decode(
                        outputs[0], skip_special_tokens=True
                    )

                subtitle["text"] = translated_text
                translated_count += 1

            end_time = time.time()
            elapsed_time = end_time - start_time
            logging.info(f"\nâœ… ç¿»è¯‘å®Œæˆï¼Œå…±ç¿»è¯‘ {translated_count} æ¡å­—å¹•")
            logging.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            logging.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(subtitles) / elapsed_time:.2f} æ¡/ç§’")

        logging.info("ğŸ’¾ æ­£åœ¨ä¿å­˜ç¿»è¯‘åçš„æ–‡ä»¶...")
        if debug_skip:
            logging.info("ğŸ è°ƒè¯•æ¨¡å¼å·²å¼€å¯: è·³è¿‡æ–‡ä»¶ä¿å­˜æ­¥éª¤ã€‚")
            # ä»…æ‰“å°æ–‡ä»¶åï¼Œä¸å®é™…ä¿å­˜
            print(f"è°ƒè¯•æ¨¡å¼ï¼šæ–‡ä»¶ {output_path} å°†è¢«ä¿å­˜ï¼Œä½†å®é™…æœªä¿å­˜ã€‚")
        else:
            write_srt_file(subtitles, output_path)

        logging.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_path}")

        logging.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logging.info(f"   - æ€»å­—å¹•è¡Œæ•°: {len(subtitles)}")
        logging.info(f"   - ç¿»è¯‘æˆåŠŸè¡Œæ•°: {translated_count}")
        logging.info(f"   - æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logging.info(f"   - å¹³å‡é€Ÿåº¦: {len(subtitles) / elapsed_time:.2f} æ¡/ç§’")

    except Exception as e:
        logging.error(f"âŒ å¤„ç†æ–‡ä»¶ {input_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False, 0

    return True, translated_count


# æ–°å¢çš„å‡½æ•°ï¼Œç”¨äºå¤„ç†æ•´ä¸ªæ–‡ä»¶å¤¹çš„æ‰¹å¤„ç†é€»è¾‘
def process_directory(input_dir, output_dir, args, tokenizer, model):
    """
    å¤„ç†æŒ‡å®šè¾“å…¥ç›®å½•ä¸‹çš„æ‰€æœ‰SRTæ–‡ä»¶ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°è¾“å‡ºç›®å½•ã€‚
    """
    if not input_dir.is_dir():
        logging.error(f"âŒ é”™è¯¯ï¼šè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return

    if not output_dir.exists():
        logging.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)

    srt_files = list(input_dir.glob("*.srt"))
    if not srt_files:
        logging.info(f"ğŸ’¡ åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• srt æ–‡ä»¶ã€‚")
        return

    logging.info(f"ğŸ” æ‰¾åˆ° {len(srt_files)} ä¸ª srt æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹æ‰¹å¤„ç†...")

    total_files = len(srt_files)
    processed_files = 0
    successful_translations = 0

    for i, input_path in enumerate(srt_files):
        try:
            output_path = get_output_file(input_path, output_dir)

            logging.info(
                f"\n--- æ­£åœ¨å¤„ç†æ–‡ä»¶ [{i + 1}/{total_files}]: {input_path.name} -> {output_path}"
            )

            success, translated_count = process_single_file(
                input_path, output_path, args, tokenizer, model
            )
            if success:
                processed_files += 1
                successful_translations += translated_count
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ–‡ä»¶ {input_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    logging.info("\n=== æ‰¹å¤„ç†å®Œæˆ ===")
    logging.info(f"   - æ€»æ–‡ä»¶æ•°: {total_files}")
    logging.info(f"   - æˆåŠŸå¤„ç†æ–‡ä»¶æ•°: {processed_files}")
    logging.info(f"   - æ€»è®¡ç¿»è¯‘è¡Œæ•°: {successful_translations}")
    logging.info("===================")


def load_model(model_type: str, model_path: str = None, auto_download: bool = False):
    """
    é€šç”¨æ¨¡å‹åŠ è½½å‡½æ•°ï¼Œæ”¯æŒopuså’Œnllbã€‚
    model_type: "opus" æˆ– "nllb"
    model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–ä¸‹è½½ç›®å½•
    auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½
    """
    model_name = TRANSLATES_MODEL_MAP.get(model_type)
    if not model_name:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    try:
        if auto_download or not model_path:
            # ä¸‹è½½åˆ°æŒ‡å®šç›®å½•æˆ–é»˜è®¤ç¼“å­˜
            cache_dir = model_path if model_path else None
            logging.info(
                f"ğŸ”„ æ­£åœ¨è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ {model_name} åˆ° {cache_dir or 'é»˜è®¤ç¼“å­˜ç›®å½•'}"
            )
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name, cache_dir=cache_dir
            )
        else:
            # åŠ è½½æœ¬åœ°æ¨¡å‹
            logging.info(f"ğŸ”’ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ {model_path}")
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_path, local_files_only=True
            )
        logging.info(f"âœ… {model_type} æ¨¡å‹åŠ è½½æˆåŠŸ")
        return tokenizer, model
    except Exception as e:
        if auto_download:
            logging.info(f"âŒ æ¨¡å‹è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
            logging.info("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            logging.info("1. æ£€æŸ¥ç½‘ç»œè¿æ¥å¹¶é‡è¯•")
            logging.info("2. æ‰‹å·¥ä¸‹è½½å¹¶æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        else:
            logging.info("âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥")
            logging.info("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            logging.info("1. ä½¿ç”¨ --auto-download å‚æ•°è‡ªåŠ¨ä¸‹è½½æ¨¡å‹")
            logging.info(f"2. ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡® {model_path}")
            logging.info("3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•")
        raise


def translate_with_opus(text: str, tokenizer, model, max_length: int = 512) -> str:
    """ä½¿ç”¨OPUS-MTæ¨¡å‹ç¿»è¯‘æ–‡æœ¬"""
    try:
        clean_text_result = clean_text(text)

        if not clean_text_result.strip():
            return text

        inputs = tokenizer(
            clean_text_result,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length,
        )
        outputs = model.generate(**inputs)
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return result
    except Exception as e:
        logging.info(f"ç¿»è¯‘æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return text


def translate_with_nllb(
    text: str,
    tokenizer,
    model,
    source_lang: str = "eng_Latn",
    target_lang: str = "zho_Hans",
    max_length: int = 512,
) -> str:
    """ä½¿ç”¨NLLBæ¨¡å‹ç¿»è¯‘æ–‡æœ¬"""
    try:
        clean_text_result = clean_text(text)

        if not clean_text_result.strip():
            return text

        input_text = f"{source_lang} {clean_text_result}"

        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length,
        )
        outputs = model.generate(
            **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(target_lang)
        )
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return result
    except Exception as e:
        logging.info(f"ç¿»è¯‘æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return text


def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="JuSubTrans - ç»Ÿä¸€SRTå­—å¹•æ–‡ä»¶AIè‡ªåŠ¨ç¿»è¯‘è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨OPUS-MTæ¨¡å‹ï¼ˆé€Ÿåº¦å¿«ï¼‰   é»˜è®¤æ¨¡å‹
  python translate.py -i input.srt
  
  # ä½¿ç”¨NLLBæ¨¡å‹ï¼ˆè´¨é‡é«˜ï¼‰
  python translate.py -di ./input_dir/ -do ./output_dir/
  
  # æŒ‡å®šæ¨¡å‹è·¯å¾„
  python translate.py -i input.srt -m opus --modelpath /path/to/model
  
  # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹   åŒæ—¶æŒ‡å®šä¸‹è½½ä½ç½®ï¼ˆéå¿…å¡«ï¼‰
  python translate.py -di ./input_dir/ -do ./output_dir/ --auto-download --modelpath /path/to/model
        """,
    )

    # åˆ›å»ºäº’æ–¥ç»„ï¼Œç”¨äºå•æ–‡ä»¶æˆ–æ‰¹å¤„ç†æ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("-i", "--input_file", help="è¾“å…¥çš„SRTæ–‡ä»¶è·¯å¾„ (å•æ–‡ä»¶æ¨¡å¼)")
    mode_group.add_argument(
        "-di", "--input_dir", help="è¾“å…¥çš„SRTæ–‡ä»¶å¤¹è·¯å¾„ (æ‰¹å¤„ç†æ¨¡å¼)"
    )

    # éäº’æ–¥çš„å‚æ•°
    parser.add_argument(
        "-o",
        "--output",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä»…å•æ–‡ä»¶æ¨¡å¼æœ‰æ•ˆï¼Œé»˜è®¤åœ¨åŸæ–‡ä»¶åååŠ æ¨¡å‹æ ‡è¯†ï¼‰",
    )
    parser.add_argument(
        "-do",
        "--output_dir",
        help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯é€‰ï¼Œä»…æ‰¹å¤„ç†æ¨¡å¼æœ‰æ•ˆï¼Œé»˜è®¤åœ¨è¾“å…¥æ–‡ä»¶å¤¹å†…åˆ›å»ºtranslatedå­ç›®å½•ï¼‰",
    )
    parser.add_argument(
        "-m",
        "--model",
        choices=["opus", "nllb"],
        default="opus",
        help="é€‰æ‹©ç¿»è¯‘æ¨¡å‹: opus(é€Ÿåº¦å¿«) æˆ– nllb(è´¨é‡é«˜) [é»˜è®¤: opus]",
    )
    parser.add_argument("--model_path", help="æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰")
    parser.add_argument(
        "--auto-download",
        default=False,
        action="store_true",
        help="å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨ä»Hugging Faceä¸‹è½½ï¼ˆé»˜è®¤ï¼šå¦ï¼‰",
    )
    parser.add_argument(
        "--source_lang",
        default="eng_Latn",
        help="æºè¯­è¨€ä»£ç ï¼ˆä»…NLLBæ¨¡å‹æœ‰æ•ˆï¼Œé»˜è®¤ï¼šeng_Latn è‹±æ–‡ï¼‰",
    )
    parser.add_argument(
        "--target_lang",
        default="zho_Hans",
        help="ç›®æ ‡è¯­è¨€ä»£ç ï¼ˆä»…NLLBæ¨¡å‹æœ‰æ•ˆï¼Œé»˜è®¤ï¼šzho_Hans ç®€ä½“ä¸­æ–‡ï¼‰",
    )
    parser.add_argument(
        "--max_length", type=int, default=512, help="æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆé»˜è®¤ï¼š512ï¼‰"
    )

    args = parser.parse_args()
    logging.debug(f"DEBUG: args = {args}")

    nomodelpath = False
    # è®¾ç½®é»˜è®¤æ¨¡å‹è·¯å¾„
    if not args.model_path:
        nomodelpath = True
        if args.model == "opus":
            args.model_path = "./models/models--Helsinki-NLP--opus-mt-en-zh/snapshots/408d9bc410a388e1d9aef112a2daba955b945255"
        else:  # nllb
            args.model_path = "./models/models--facebook--nllb-200-distilled-600M/snapshots/f8d333a098d19b4fd9a8b18f94170487ad3f821d"

    # æ‰“å°ä¸€äº›åŸºæœ¬ä¿¡æ¯
    logging.info("ğŸ¯ JuSubTitleAutoTranslate - å­—å¹•ç¿»è¯‘å·¥å…·")
    logging.info(f"ğŸ¤– æ¨¡å‹: {args.model}")
    logging.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {args.model_path}")
    logging.info(f"â¬‡ï¸  è‡ªåŠ¨ä¸‹è½½: {'æ˜¯' if args.auto_download else 'å¦'}")
    argmodelpath = Path(args.model_path)
    if args.auto_download:
        if nomodelpath:
            argmodelpath = "./models/"
        logging.info(f"ğŸ“ ä¸‹è½½ç›®å½•: {argmodelpath}")

    # å¤„ç† NLLB æ¨¡å‹çš„è¯­è¨€ä»£ç 
    if args.model == "nllb":
        logging.info(f"ğŸŒ æºè¯­è¨€: {args.source_lang}")
        logging.info(f"ğŸŒ ç›®æ ‡è¯­è¨€: {args.target_lang}")
        # 1. æ£€æŸ¥æºè¯­è¨€æ˜¯å¦å·²ç»æ˜¯NLLBå®˜æ–¹ä»£ç 
        if args.source_lang.lower() not in nllb_language_codes:
            logging.error(
                f"âš ï¸ æ— æ³•è¯†åˆ«çš„è¯­è¨€ä»£ç : {args.source_lang}ï¼Œæ ¹æ®nllbæ¨¡å‹è§„å®šè¯·ä½¿ç”¨ISO 639-2/T + ISO 15924 æ ¼å¼çš„è¯­è¨€ä»£ç ã€‚"
            )
            return
        # 2. æ£€æŸ¥ç›®æ ‡è¯­è¨€æ˜¯å¦å·²ç»æ˜¯NLLBå®˜æ–¹ä»£ç 
        if args.target_lang.lower() not in nllb_language_codes:
            logging.error(
                f"âš ï¸ æ— æ³•è¯†åˆ«çš„è¯­è¨€ä»£ç : {args.source_lang}ï¼Œæ ¹æ®nllbæ¨¡å‹è§„å®šè¯·ä½¿ç”¨ISO 639-2/T + ISO 15924 æ ¼å¼çš„è¯­è¨€ä»£ç ã€‚"
            )
            return

    # 2. ç»Ÿä¸€åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer, model = None, None
    if debug_skip:
        logging.info("ğŸ è°ƒè¯•æ¨¡å¼å·²å¼€å¯: è·³è¿‡æ¨¡å‹åŠ è½½å’Œç¿»è¯‘ã€‚")
    else:
        try:
            tokenizer, model = load_model(args.model, argmodelpath, args.auto_download)
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

    # 3. æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å¼å¹¶æ‰§è¡Œ
    if args.input_file:
        # å•æ–‡ä»¶æ¨¡å¼
        input_path = Path(args.input_file)
        if not validate_file_format(input_path):
            logging.error(f"âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶æ ¼å¼æš‚ä¸æ”¯æŒ: {input_path}")
            sys.exit(1)

        output_path = get_output_file(input_path, args.output)

        logging.info("--- è¿›å…¥å•æ–‡ä»¶ç¿»è¯‘æ¨¡å¼ ---")
        logging.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
        logging.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")

        success, _ = process_single_file(
            input_path, output_path, args, tokenizer, model
        )
        if success:
            logging.info(f"ğŸ‰ ç¿»è¯‘å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")

    elif args.input_dir:
        # æ‰¹å¤„ç†æ¨¡å¼
        input_dir = Path(args.input_dir)
        output_dir = (
            Path(args.output_dir) if args.output_dir else input_dir / "translated"
        )

        logging.info("--- è¿›å…¥æ‰¹å¤„ç†ç¿»è¯‘æ¨¡å¼ ---")
        logging.info(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        logging.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        process_directory(input_dir, output_dir, args, tokenizer, model)

    else:
        # æ­¤å¤„ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œï¼Œå› ä¸º argparse çš„ required=True å·²ç»å¼ºåˆ¶ç”¨æˆ·æä¾›äº†å‚æ•°
        logging.error("âŒ é”™è¯¯ï¼šè¯·æä¾› -i æˆ– -di å‚æ•°")
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logging.info("Starting JuSubTitleAutoTranslate...")
    main()
